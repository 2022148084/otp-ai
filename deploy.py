import modal

# [Helper] ë¹Œë“œ íƒ€ì„ ë‹¤ìš´ë¡œë“œ (CPU ëª¨ë“œ)
def download_models():
    import os
    os.system("mkdir -p /root/weights")
    os.system("wget -O /root/weights/network-sintel-final.pytorch http://content.sniklaus.com/github/pytorch-spynet/network-sintel-final.pytorch")
    os.system("wget -O /root/weights/insta_kakao_final_agent_model_pytorch.pth https://github.com/YeeDEA/kakao_insta_detector/raw/refs/heads/main/final_agent_model_pytorch.pth")
    
    from paddleocr import PaddleOCR
    print("â¬‡ï¸ Downloading PaddleOCR models (CPU build)...")
    # ë¹Œë“œ ì‹œì ì—” CPUë¡œ ë‹¤ìš´ë¡œë“œë§Œ ìˆ˜í–‰
    PaddleOCR(lang="korean", use_angle_cls=False, show_log=False, use_gpu=False)

# 1. í™˜ê²½ ì„¤ì •
image = (
    modal.Image.from_registry("nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04", add_python="3.10")
    .apt_install("libgl1-mesa-glx", "libglib2.0-0", "wget")
    .pip_install(
        "torch", 
        "torchvision",
        extra_options="--index-url https://download.pytorch.org/whl/cu118"
    )
    .pip_install(
        "paddlepaddle-gpu==2.6.1",
        "paddleocr==2.7.3",
        "opencv-python-headless",
        "numpy<2.0.0",
        "tqdm"
    )
    .run_function(download_models)
)

app = modal.App("kakao-ocr-unified")

@app.cls(image=image, gpu="T4", scaledown_window=100, min_containers=0)
class OCRService:
    
    @modal.enter()
    def initialize(self):
        import torch
        import torch.nn as nn
        from paddleocr import PaddleOCR
        import os
        from torchvision import models
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"\nğŸ‘‰ Current Computing Device: {self.device}\n")

        # --- í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ëª¨ë¸ ì •ì˜ ---
        model_path = "/root/weights/insta_kakao_final_agent_model_pytorch.pth"
        
        try:
            if os.path.exists(model_path):
                # 1. ë¼ˆëŒ€ ë¡œë“œ (MobileNetV2)
                self.classifier = models.mobilenet_v2(weights=None)
                
                # 2. Classifier êµ¬ì¡° êµì²´ (í•™ìŠµ ì½”ë“œì™€ 100% ì¼ì¹˜ì‹œí‚´)
                # êµ¬ì¡°: Dropout(0.3) -> Linear(1280, 64) -> ReLU -> Linear(64, 1)
                self.classifier.classifier = nn.Sequential(
                    nn.Dropout(p=0.3),
                    nn.Linear(1280, 64),
                    nn.ReLU(),
                    nn.Linear(64, 1)
                )
                
                # 3. ê°€ì¤‘ì¹˜ ë¡œë“œ
                state_dict = torch.load(model_path, map_location=self.device)
                self.classifier.load_state_dict(state_dict)
                self.classifier.to(self.device).eval()
                print("âœ… Custom MobileNetV2 Loaded Successfully!")
            else:
                print("âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                self.classifier = None
                
        except Exception as e:
            print(f"âš ï¸ Failed to load classifier: {e}")
            self.classifier = None
        
        # --- SPyNet ì •ì˜ ---
        class SPyNet(torch.nn.Module):
            def __init__(self):
                super().__init__()
                class Preprocess(torch.nn.Module):
                    def forward(self, tenInput):
                        tenInput = tenInput.flip([1])
                        tenInput = tenInput - torch.tensor([0.485,0.456,0.406], dtype=tenInput.dtype, device=tenInput.device).view(1,3,1,1)
                        tenInput = tenInput * torch.tensor([1/0.229,1/0.224,1/0.225], dtype=tenInput.dtype, device=tenInput.device).view(1,3,1,1)
                        return tenInput
                class Basic(torch.nn.Module):
                    def __init__(self):
                        super().__init__()
                        self.netBasic = torch.nn.Sequential(
                            torch.nn.Conv2d(8,32,7,1,3), torch.nn.ReLU(False),
                            torch.nn.Conv2d(32,64,7,1,3), torch.nn.ReLU(False),
                            torch.nn.Conv2d(64,32,7,1,3), torch.nn.ReLU(False),
                            torch.nn.Conv2d(32,16,7,1,3), torch.nn.ReLU(False),
                            torch.nn.Conv2d(16,2,7,1,3)
                        )
                    def forward(self, x): return self.netBasic(x)
                self.netPreprocess = Preprocess()
                self.netBasic = torch.nn.ModuleList([Basic() for _ in range(6)])
            def forward(self, tenOne, tenTwo):
                tenOne_list = [self.netPreprocess(tenOne)]
                tenTwo_list = [self.netPreprocess(tenTwo)]
                for _ in range(5):
                    if tenOne_list[0].shape[2] > 32:
                        tenOne_list.insert(0, torch.nn.functional.avg_pool2d(tenOne_list[0],2))
                        tenTwo_list.insert(0, torch.nn.functional.avg_pool2d(tenTwo_list[0],2))
                tenFlow = tenOne_list[0].new_zeros([1,2,tenOne_list[0].shape[2]//2, tenOne_list[0].shape[3]//2])
                for i in range(len(tenOne_list)):
                    tenUpsampled = torch.nn.functional.interpolate(tenFlow, scale_factor=2, mode="bilinear", align_corners=True) * 2.0
                    if tenUpsampled.shape[2] != tenOne_list[i].shape[2]: tenUpsampled = torch.nn.functional.pad(tenUpsampled,[0,0,0,1])
                    if tenUpsampled.shape[3] != tenOne_list[i].shape[3]: tenUpsampled = torch.nn.functional.pad(tenUpsampled,[0,1,0,0])
                    tenInput = tenTwo_list[i]
                    tenFlow_warp = tenUpsampled
                    H, W = tenFlow_warp.shape[2], tenFlow_warp.shape[3]
                    tenHor = torch.linspace(-1.0, 1.0, W, device=tenFlow_warp.device).view(1,1,1,W).repeat(1,1,H,1)
                    tenVer = torch.linspace(-1.0, 1.0, H, device=tenFlow_warp.device).view(1,1,H,1).repeat(1,1,1,W)
                    tenGrid = torch.cat([tenHor, tenVer], 1)
                    tenFlowNorm = torch.cat([tenFlow_warp[:,0:1]*(2.0/(tenInput.shape[3]-1.0)), tenFlow_warp[:,1:2]*(2.0/(tenInput.shape[2]-1.0))], 1)
                    warped = torch.nn.functional.grid_sample(tenInput, (tenGrid + tenFlowNorm).permute(0,2,3,1), mode="bilinear", padding_mode="reflection", align_corners=True)
                    tenFlow = self.netBasic[i](torch.cat([tenOne_list[i], warped, tenUpsampled], 1)) + tenUpsampled
                return tenFlow
        
        self.spynet = SPyNet().to(self.device).eval()
        
        weight_path = "/root/weights/network-sintel-final.pytorch"
        if os.path.exists(weight_path):
            state_dict = torch.load(weight_path, map_location="cpu")
            self.spynet.load_state_dict({k.replace("module","net"):v for k,v in state_dict.items()})
        
        self.ocr = PaddleOCR(lang="korean", use_angle_cls=False, show_log=False, use_gpu=True)
        print("âœ… Service Ready")

    # ----------------------------------------------------------------
    # ë¶„ë¥˜ í•¨ìˆ˜ (ì¶œë ¥ í˜•ì‹ ìˆ˜ì •ë¨: 'kakao' or 'insta')
    # ----------------------------------------------------------------
    def classify_image(self, img_bgr):
        import cv2
        import numpy as np
        import torch

        if self.classifier is None:
            return "kakao" # ëª¨ë¸ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì • (í˜¹ì€ ì—ëŸ¬ ì²˜ë¦¬)

        # [í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬]
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (224, 224))
        
        # Normalize
        img_float = img_resized.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_norm = (img_float - mean) / std
        
        img_t = img_norm.transpose(2, 0, 1)
        
        # .float()ë¡œ ê°•ì œ í˜•ë³€í™˜
        tensor = torch.from_numpy(img_t).unsqueeze(0).to(self.device).float()

        with torch.no_grad():
            output = self.classifier(tensor)
            prob = torch.sigmoid(output).item()

        # [ìˆ˜ì •ë¨] ë‹¨ìˆœ ë¬¸ìì—´ ë°˜í™˜
        if prob > 0.5:
            return "kakao"
        else:
            return "insta"

    def estimate_flow(self, tenOne, tenTwo):
        import torch
        H, W = tenOne.shape[1:]
        Hp, Wp = (H + 31) // 32 * 32, (W + 31) // 32 * 32
        tenOne = torch.nn.functional.interpolate(tenOne.unsqueeze(0), (Hp, Wp))
        tenTwo = torch.nn.functional.interpolate(tenTwo.unsqueeze(0), (Hp, Wp))
        with torch.no_grad():
            flow = torch.nn.functional.interpolate(self.spynet(tenOne, tenTwo), size=(H, W))
        flow[:,0] *= float(W)/float(Wp)
        flow[:,1] *= float(H)/float(Hp)
        return flow[0].cpu().numpy()
    
    # ----------------------------------------------------------------
    # ì¸ìŠ¤íƒ€ìš© íŒŒì‹± í•¨ìˆ˜
    # ----------------------------------------------------------------
    def parse_ocr_result_insta(self, ocr_result, image_width):
        import re
        if not ocr_result: return ""
        
        # 1. ì›ë³¸ ë°ì´í„° ì •ë ¬ (Yì¶• ê¸°ì¤€)
        example_polys = [line[0] for line in ocr_result]
        example_texts = [line[1][0] for line in ocr_result]
        
        max_y = 0
        if example_polys:
            max_y = max([max([p[1] for p in poly]) for poly in example_polys])
            
        all_items = []
        for i in range(len(example_texts)):
            text, box = example_texts[i], example_polys[i]
            y_coords, x_coords = [p[1] for p in box], [p[0] for p in box]
            y_center = (min(y_coords)+max(y_coords))/2
            
            all_items.append({
                'text': text, 
                'y_center': y_center,
                'x_left': min(x_coords), 
                'x_right': max(x_coords),
            })
        
        all_items.sort(key=lambda x: x['y_center'])

        all_items.sort(key=lambda x: x['y_center'])

        # ---------------------------------------------------------
        # [Step 1] í—¤ë” ë¶„ì„: ì´ë¦„(Y) ê¸°ì¤€ìœ¼ë¡œ ê°€ê¹Œìš´ í•˜ë‹¨ í…ìŠ¤íŠ¸(ID/ìƒíƒœ) ëª¨ë‘ ë¬´ì‹œ
        # ---------------------------------------------------------
        opponent_name = "ìƒëŒ€ë°©"
        start_idx = 0            
        noise_pattern = re.compile(r"^(\d{1,2}:\d{2}|\d+%|\d+)$")
        
        found_name = False
        name_y_center = 0
        
        # í—¤ë”ë¡œ ê°„ì£¼í•  Yì¶• ê±°ë¦¬ ì„ê³„ê°’ (ì´ë¦„ ë°”ë¡œ ë°‘ì— ë¶™ì–´ìˆëŠ” ê²ƒë“¤ì€ ë¬´ì‹œ)
        # ë³´í†µ IDë‚˜ ìƒíƒœë©”ì‹œì§€ëŠ” ì´ë¦„ê³¼ 100px ì´ë‚´ì— ë¶™ì–´ìˆìŠµë‹ˆë‹¤.
        HEADER_MARGIN = 120 
        
        for i, item in enumerate(all_items):
            text = item['text']
            
            # 1. ìƒë‹¨ ë…¸ì´ì¦ˆ íŒ¨ìŠ¤
            if noise_pattern.match(text): continue
            
            if not found_name:
                # ì´ë¦„ì„ ì°¾ìŒ
                opponent_name = text
                found_name = True
                name_y_center = item['y_center']
                continue 
            
            # 2. ì´ë¦„ì„ ì°¾ì€ í›„:
            # í˜„ì¬ ì•„ì´í…œì˜ Yì¢Œí‘œê°€ ì´ë¦„ê³¼ ë„ˆë¬´ ê°€ê¹ë‹¤ë©´(í—¤ë” ì˜ì—­) ë¬´ì‹œí•˜ê³  ë„˜ì–´ê°
            if found_name and (item['y_center'] - name_y_center < HEADER_MARGIN):
                continue

            # 3. Y ê±°ë¦¬ ì°¨ì´ê°€ ì¶©ë¶„íˆ ë²Œì–´ì¡Œë‹¤ë©´ ì—¬ê¸°ì„œë¶€í„°ê°€ ì‹¤ì œ ì±„íŒ…
            start_idx = i
            break

        chat_items = all_items[start_idx:]
        
        # ---------------------------------------------------------
        # [Step 2] í‘¸í„° ë¶„ì„: í•˜ë‹¨ ì…ë ¥ì°½ ì œê±°
        # ---------------------------------------------------------
        input_area_threshold = max_y * 0.90 
        chat_items = [item for item in chat_items if item['y_center'] < input_area_threshold]

        # ---------------------------------------------------------
        # [Step 3] ì±„íŒ… ë¼ì¸ ê·¸ë£¹í™”
        # ---------------------------------------------------------
        time_regex = re.compile(r"(ì˜¤ì „|ì˜¤í›„)?\s*\d{1,2}[:ì‹œ]\s?\d{2}|^\d{4}ë…„|\d{1,2}ì›”\s?\d{1,2}ì¼")

        for item in chat_items:
            item['is_timestamp'] = bool(time_regex.search(item['text']))

        final_lines = []
        current_line = []
        if chat_items:
            base_y = chat_items[0]['y_center']
            for item in chat_items:
                if abs(item['y_center'] - base_y) < 20:
                    current_line.append(item)
                else:
                    current_line.sort(key=lambda x: x['x_left'])
                    final_lines.append(current_line)
                    current_line = [item]
                    base_y = item['y_center']
            if current_line:
                current_line.sort(key=lambda x: x['x_left'])
                final_lines.append(current_line)

        # ---------------------------------------------------------
        # [Step 4] íŒŒì‹± ë° ì„ì‹œ ì €ì¥ (í›„ì²˜ë¦¬ë¥¼ ìœ„í•´ êµ¬ì¡°ì²´ë¡œ ì €ì¥)
        # ---------------------------------------------------------
        temp_logs = []  # ë¬¸ìì—´ ëŒ€ì‹  ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
        center_x = image_width / 2
        
        DEFAULT_TIME = "2025. 12. 3. 04:56"
        last_timestamp = DEFAULT_TIME

        for line_items in final_lines:
            # 1) íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹ 
            timestamps = [x['text'] for x in line_items if x['is_timestamp']]
            if timestamps:
                last_timestamp = timestamps[-1]

            # 2) ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
            msg_texts = [x['text'] for x in line_items if not x['is_timestamp']]
            if not msg_texts: continue
            
            full_msg = " ".join(msg_texts)

            # 3) í™”ì êµ¬ë¶„
            first_item = next(x for x in line_items if not x['is_timestamp'])
            if first_item['x_left'] > center_x - (image_width * 0.1): 
                speaker = "ë‚˜"
            else:
                speaker = opponent_name

            # **ì¤‘ìš”: ë°”ë¡œ ë¬¸ìì—´ë¡œ ë§Œë“¤ì§€ ì•Šê³  ë°ì´í„°ë¡œ ì €ì¥**
            temp_logs.append({
                'time': last_timestamp,
                'speaker': speaker,
                'msg': full_msg
            })

        # ---------------------------------------------------------
        # [Step 5] í›„ì²˜ë¦¬: íƒ€ì„ìŠ¤íƒ¬í”„ ì—­ë°©í–¥ ì±„ìš°ê¸° (Backfill)
        # ---------------------------------------------------------
        # 1. ì „ì²´ ë¡œê·¸ ì¤‘ '2000ë…„...'ì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ìœ íš¨ ì‹œê°„ì„ ì°¾ìŒ
        first_valid_time = DEFAULT_TIME
        for log in temp_logs:
            if log['time'] != DEFAULT_TIME:
                first_valid_time = log['time']
                break
        
        # 2. ìœ íš¨ ì‹œê°„ì´ ë°œê²¬ë˜ì—ˆë‹¤ë©´, ì•ë¶€ë¶„ì˜ ë¯¸ìƒ ì‹œê°„ë“¤ì„ ëª¨ë‘ ì´ ì‹œê°„ìœ¼ë¡œ ë®ì–´ì”€
        if first_valid_time != DEFAULT_TIME:
            for log in temp_logs:
                if log['time'] == DEFAULT_TIME:
                    log['time'] = first_valid_time
                else:
                    # ìœ íš¨í•œ ì‹œê°„ì„ ë§Œë‚˜ë©´(ì´ë¯¸ ì •ìƒì´ë¯€ë¡œ) ë£¨í”„ ì¤‘ë‹¨
                    break

        # ---------------------------------------------------------
        # [Step 6] ìµœì¢… ë¬¸ìì—´ ë³€í™˜
        # ---------------------------------------------------------
        return "\n".join([f"{log['time']}, {log['speaker']} : {log['msg']}" for log in temp_logs])

    def parse_ocr_result_kakao(self, ocr_result, image_width, image_height):
        if not ocr_result: return ""
        import re

        # ==============================================================================
        # ë°ì´í„° ì •ì œ ë° íƒ€ì„ìŠ¤íƒ¬í”„/ë…¸ì´ì¦ˆ ë¶„ë¥˜
        # ==============================================================================
        
        def get_timestamp_token(text):
            # 1. êµ¬ë¶„ì ì •ê·œí™” (ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ íŒ¨í„´ ëŒ€ì‘)
            normalized = re.sub(r'[.,-]', ':', text)
            
            # 2. ìˆ«ìì™€ ì½œë¡ ë§Œ ë‚¨ê¸°ê³  ì¶”ì¶œ
            clean_nums = re.sub(r"[^\d:]", "", normalized)

            # 3. [ì „ëµ A] ì™„ë²½í•œ í¬ë§· (Strict Match)
            if re.match(r"^(\d{1,2}):(\d{2})$", clean_nums):
                return clean_nums

            # 4. [ì „ëµ B] ê¹¨ì§„ íƒ€ì„ìŠ¤íƒ¬í”„ í›„ë³´êµ° (Loose Match) -> "xx:xx" ë³€í™˜
            if ':' in normalized and re.search(r'\d', normalized):
                if len(normalized) <= 8:
                    return "xx:xx"
            
            return None

        items = []
        for line in ocr_result:
            box = line[0]
            text = line[1][0]
            
            # í•„í„°ë§ ë¦¬ìŠ¤íŠ¸
            if text in ['<', '>', '-', '=', 'íŒŒì‹±ìš©', 'ë©”ì‹œì§€ì…ë ¥', 'ì „ì†¡', 'ì¹´í†¡', 'ëŒ€í™”', '|', 'emoticon']: continue
            if re.match(r"^\d{1,3}%?$", text): continue 

            # (ê¸°ì¡´ ì¢Œí‘œ í•„í„°ë§ ë¡œì§ ì œê±° - Step 2 ì´í›„ë¡œ ì´ë™)

            ts_token = get_timestamp_token(text)
            
            xs = [p[0] for p in box]
            ys = [p[1] for p in box]
            
            items.append({
                'text': text, 
                'clean_text': text.replace(" ", ""),
                'ts_token': ts_token,
                'y_center': sum(ys)/len(ys),
                'x_left': min(xs),
                'height': max(ys) - min(ys)
            })

        items.sort(key=lambda x: x['y_center'])

        # ==============================================================================
        # ë¼ì¸ ê·¸ë£¹í™” (Tough Rule)
        # ==============================================================================
        
        lines = []
        if items:
            current_line = [items[0]]
            base_y = items[0]['y_center']
            
            for item in items[1:]:
                if abs(item['y_center'] - base_y) < (item['height'] * 0.7 + 5):
                    current_line.append(item)
                else:
                    current_line.sort(key=lambda x: x['x_left'])
                    lines.append(current_line)
                    current_line = [item]
                    base_y = item['y_center']
            
            if current_line:
                current_line.sort(key=lambda x: x['x_left'])
                lines.append(current_line)

        # ==============================================================================
        # ì˜ì—­ ê¸°ë°˜ í•„í„°ë§ (ë¼ì¸ ë³‘í•© í›„ ì²˜ë¦¬)
        # ==============================================================================
        # ê°œë³„ ê¸€ìê°€ ì•„ë‹Œ 'ì™„ì„±ëœ ì¤„' ë‹¨ìœ„ë¡œ ìœ„ì¹˜ë¥¼ íŒë‹¨í•˜ì—¬ ì œê±°í•©ë‹ˆë‹¤.
        
        if image_height > 0:
            filtered_lines = []
            for line in lines:
                # í•´ë‹¹ ë¼ì¸ì˜ ëŒ€í‘œê°’ ê³„ì‚° (í‰ê·  yê°’, ê°€ì¥ ì™¼ìª½ xê°’)
                line_y_center = sum([item['y_center'] for item in line]) / len(line)
                line_x_left = min([item['x_left'] for item in line])
                
                # 1) ìƒë‹¨ 1/8 ì œê±° (ì‹œìŠ¤í…œ ì˜ì—­)
                if line_y_center < image_height / 15: continue
                
                # 2) í•˜ë‹¨ 1/12 ì œê±° (ë©”ì‹œì§€ ì…ë ¥ì°½)
                if line_y_center > image_height * (14/15): continue
                
                # 3) ìƒë‹¨ 1/4 ì´ë©´ì„œ ì¢Œì¸¡ 1/6 ì˜ì—­ (ê³µì§€ì‚¬í•­ ì•„ì´ì½˜ ë“±)
                if line_y_center < image_height / 4 and line_x_left <= image_width / 9 : continue
                
                filtered_lines.append(line)
            lines = filtered_lines

        # ==============================================================================
        # ë¬¸ë§¥ íŒŒì‹± (ë™ì  í™”ì ë¡œì§ & í•„í„°ë§ ì¶”ê°€)
        # ==============================================================================
        
        parsed_logs = []
        known_speakers = set(["ë‚˜"]) 
        last_left_speaker = "ì•Œìˆ˜ì—†ìŒ"
        current_date = "2025. 12. 5." 
        center_x = image_width / 2 if image_width else 200

        i = 0
        while i < len(lines):
            line = lines[i]
            full_line_str = " ".join([x['text'] for x in line])
            
            # ë‹µì¥ ë¡œì§ ì œê±° ("ì—ê²Œ ë‹µì¥"ì´ í¬í•¨ëœ ì¤„ê³¼ ê·¸ ë‹¤ìŒ ì¤„ ìŠ¤í‚µ)
            if "ì—ê²Œ ë‹µì¥" in full_line_str:
                i += 2
                continue

            # ë‚ ì§œ í—¤ë” ì²˜ë¦¬
            # ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ì—¬ current_dateë¥¼ ê°±ì‹ í•˜ë˜, parsed_logsì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ(ì¶œë ¥ ì œì™¸)
            date_match = re.search(r"20\d{2}[^0-9]+\d{1,2}[^0-9]+\d{1,2}", full_line_str)
            if date_match:
                nums = re.findall(r"\d+", full_line_str)
                if len(nums) >= 3:
                    current_date = f"{nums[0]}. {nums[1]}. {nums[2]}."
                i += 1
                continue

            # [ë¼ì¸ ìš”ì†Œ ë¶„í•´]
            time_obj = None
            content_texts = []
            
            avg_x = sum([x['x_left'] for x in line]) / len(line)
            is_me = avg_x > center_x

            for item in line:
                if item['ts_token']:
                    time_obj = item['ts_token']
                    continue
                content_texts.append(item['text'])

            clean_content = " ".join(content_texts)
            
            # ë¹ˆ ë‚´ìš©ì´ë©´ ìŠ¤í‚µ
            if not clean_content: 
                i += 1
                continue

            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±° ë¡œì§
            # ì œê±° ëŒ€ìƒ í‚¤ì›Œë“œ ì •ì˜
            sys_remove_keywords = ["ì´ˆëŒ€í–ˆìŠµë‹ˆë‹¤", "ë“¤ì–´ì™”ìŠµë‹ˆë‹¤", "ë‚˜ê°”ìŠµë‹ˆë‹¤", "ì›ì„ ë³´ëƒˆì–´ìš”", "í–‰ìš´ì˜ ì£¼ì¸ê³µ"]
            
            # í™”ì íŒŒì•…ì„ ìœ„í•´ ì´ˆëŒ€/ë‚˜ê° ë©”ì‹œì§€ëŠ” ë¶„ì„ì´ í•„ìš”í•¨ (DBê°±ì‹ ìš©)
            if "ì´ˆëŒ€í–ˆìŠµë‹ˆë‹¤" in clean_content or "ë‚˜ê°”ìŠµë‹ˆë‹¤" in clean_content or "ë“¤ì–´ì™”ìŠµë‹ˆë‹¤" in clean_content:
                names = re.findall(r"([ê°€-í£a-zA-Z0-9]+)ë‹˜", clean_content)
                for n in names: known_speakers.add(n)
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë¡œê·¸ì— ì¶”ê°€í•˜ì§€ ì•Šê³  ìŠ¤í‚µ (ì¶œë ¥ ì œì™¸)
            if any(keyword in clean_content for keyword in sys_remove_keywords):
                i += 1
                continue

            # [í™”ì ë° ë©”ì‹œì§€ ì²˜ë¦¬]
            final_speaker = None
            is_name_tag_line = False

            if is_me:
                final_speaker = "ë‚˜"
            else:
                txt_nospace = clean_content.replace(" ", "")
                
                if txt_nospace in known_speakers:
                    is_name_tag_line = True
                elif 2 <= len(txt_nospace) <= 6 and re.match(r"^[ê°€-í£a-zA-Z0-9]+$", txt_nospace):
                    ending_checker = txt_nospace[-1]
                    msg_indicators = ['ë‹¤', 'ìš”', 'ìŒ', 'ëŠ”', 'ê²Œ', 'ì§€', 'ë„¤', 'ê°€', 'ë‚˜', 'ì–´', 'ã…‹', 'ã…', '?', '!']
                    
                    if ending_checker in msg_indicators:
                        is_name_tag_line = False
                    else:
                        is_name_tag_line = True
                        known_speakers.add(txt_nospace)

                if is_name_tag_line:
                    last_left_speaker = clean_content 
                    i += 1
                    continue 
                else:
                    final_speaker = last_left_speaker

            # ë©”ì‹œì§€ì— í˜„ì¬ ë‚ ì§œ(current_date)ë¥¼ í•¨ê»˜ ì €ì¥í•˜ì—¬ ì •í™•í•œ ë‚ ì§œ í‘œê¸° ë³´ì¥
            parsed_logs.append({
                'type': 'msg',
                'speaker': final_speaker,
                'text': clean_content,
                'time': time_obj,
                'date': current_date 
            })
            
            i += 1

        # ==============================================================================
        # ì‹œê°„ ì—­ì „íŒŒ (Back-fill)
        # ==============================================================================
        
        final_lines = []
        future_time = "10:22"
        
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ë°œê²¬ëœ "ìœ íš¨í•œ" ì‹œê°„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        for log in reversed(parsed_logs):
            t = log.get('time')
            if t and t != "xx:xx" and ":" in t:
                future_time = t
                break
        
        for log in reversed(parsed_logs):
            curr_t = log.get('time')
            # í˜„ì¬ ì‹œê°„ì´ ìœ íš¨í•˜ë©´ future_timeì„ ê°±ì‹ 
            if curr_t and curr_t != "xx:xx" and ":" in curr_t:
                future_time = curr_t
            
            display_time = future_time
            msg_date = log.get('date', "2025. 12. 5.") # ì €ì¥ëœ ë‚ ì§œ ì‚¬ìš©
            
            final_lines.append(f"{msg_date} {display_time}, {log['speaker']} : {log['text']}")

        final_lines.reverse()
        return "\n".join(final_lines)

    def _flush_turn(self, current_turn, chat_logs, ts_str, center_x):
        first_line = current_turn[0]
        l_center = (min(x['x_left'] for x in first_line) + max(x['x_right'] for x in first_line)) / 2
        speaker = "ë‚˜" if l_center > center_x else " ".join([x['text'] for x in first_line])
        start_idx = 0 if speaker == "ë‚˜" or len(current_turn) == 1 else 1
        msgs = [" ".join([x['text'] for x in row]) for row in current_turn[start_idx:]]
        full_msg = " ".join(msgs)
        if full_msg: chat_logs.append(f"{ts_str}, {speaker} : {full_msg}")

    # =================================================================
    # API Methods
    # =================================================================
    @modal.method()
    def process_image(self, image_bytes: bytes):
        import cv2
        import numpy as np
        nparr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img is None: return "Error: Invalid image."
        
        cls_result = self.classify_image(img)
        
        result = self.ocr.ocr(img, cls=False)
        if not result or not result[0]: return f"[{cls_result}] OCR ê²°ê³¼ ì—†ìŒ"
        
        # [ìˆ˜ì •ë¨] ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ë¥¸ íŒŒì‹± í•¨ìˆ˜ í˜¸ì¶œ
        if cls_result == "kakao":
            ocr_text = self.parse_ocr_result_kakao(result[0], img.shape[1], img.shape[0])
        else:
            # 'insta'ì¼ ê²½ìš°
            ocr_text = self.parse_ocr_result_insta(result[0], img.shape[1])
            
        return f"--- ë¶„ë¥˜ ê²°ê³¼: {cls_result} ---\n\n{ocr_text}"
    
    @modal.method()
    def process_video(self, video_bytes: bytes):
        import cv2
        import numpy as np
        import torch
        import os
        
        temp_path = "/tmp/input_video.mp4"
        with open(temp_path, "wb") as f:
            f.write(video_bytes)
            
        cap = cv2.VideoCapture(temp_path)
        if not cap.isOpened(): return {"text": "Error: Cannot open video."}

        print("ğŸ¥ Extracting keyframes (Original Logic - No Skip)...")
        ret, prev = cap.read()
        if not ret: return {"text": "Error: Empty video."}
        
        prev_rgb = cv2.cvtColor(prev, cv2.COLOR_BGR2RGB)
        h, w = prev_rgb.shape[:2]
        
        scroll_acc = 0
        extracted_frames_rgb = [prev_rgb]
        
        # [ë³µêµ¬ë¨] í”„ë ˆì„ ìŠ¤í‚µ ì—†ì´ ëª¨ë“  í”„ë ˆì„ ê²€ì‚¬
        while True:
            ret, frame = cap.read()
            if not ret: break
            
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            prev_crop = prev_rgb[h//4 : h*3//4, w//4 : w*3//4]
            curr_crop = frame_rgb[h//4 : h*3//4, w//4 : w*3//4]
            
            tenOne = torch.from_numpy(prev_crop.transpose(2,0,1).copy()).float().to(self.device) / 255.0
            tenTwo = torch.from_numpy(curr_crop.transpose(2,0,1).copy()).float().to(self.device) / 255.0
            
            flow = self.estimate_flow(tenOne, tenTwo)
            dy = np.median(flow[:,:,1])
            
            if abs(dy) < 0.3: dy = 0
            scroll_acc += dy
            
            # ì„ê³„ê°’ì€ íŒ€ì› ì›ë˜ ì½”ë“œ(0.75) ìœ ì§€
            if abs(scroll_acc) > h * 0.75:
                extracted_frames_rgb.append(frame_rgb)
                scroll_acc = 0
                print(f"ğŸ“¸ Captured frame {len(extracted_frames_rgb)}")
            
            prev_rgb = frame_rgb
            
        cap.release()
        
        print(f"ğŸ“ Running OCR on {len(extracted_frames_rgb)} frames...")
        all_logs = []

        # [ì¶”ê°€ë¨] ì˜ìƒ ë¶„ë¥˜ ë¡œì§ (ê¸°ë³¸ê°’ kakao)
        cls_result = ""
        if extracted_frames_rgb:
            first_frame_bgr = cv2.cvtColor(extracted_frames_rgb[0], cv2.COLOR_RGB2BGR)
            cls_result = self.classify_image(first_frame_bgr)
            all_logs.append(f"--- [ì˜ìƒ ë¶„ë¥˜ ê²°ê³¼: {cls_result}] ---\n")

        for frame in extracted_frames_rgb:
            result = self.ocr.ocr(frame, cls=False)
            if not result or not result[0]: continue
            
            # [ìˆ˜ì •ë¨] ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ë¥¸ íŒŒì‹± í•¨ìˆ˜ í˜¸ì¶œ
            if cls_result == "kakao":
                parsed_text = self.parse_ocr_result_kakao(result[0], w, h)
            else:
                # 'insta'ì¼ ê²½ìš°
                parsed_text = self.parse_ocr_result_insta(result[0], w)
                
            if parsed_text:
                all_logs.append(parsed_text)
                
        return {"text": "\n".join(all_logs)}

# --- ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ---
@app.local_entrypoint()
def main(file_path: str = "test_video.mp4"):
    service = OCRService()
    try:
        with open(file_path, "rb") as f:
            data = f.read()
        
        print(f"ğŸš€ Sending {file_path} to Modal...")
        if file_path.endswith(('.mp4', '.mov')):
            result = service.process_video.remote(data)
            print("\n--- [Video Result] ---")
            print(result.get("text"))
        else:
            result = service.process_image.remote(data)
            print("\n--- [Image Result] ---")
            print(result)
            
    except FileNotFoundError:
        print(f"âŒ File not found: {file_path}")